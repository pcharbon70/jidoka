**Context Management in LLM-Powered Coding Work-Sessions**

Effective context management is **crucial** for LLM-based coding assistants like _JidoCode_, which maintain multiple sessions across multiple projects. Providing too much context can overwhelm the model with irrelevant details (hurting output quality and costs), while too little context can cause it to lose important information and state. Recent studies confirm that as an LLM's context grows, models often struggle to utilize all the information effectively[blog.jetbrains.com](https://blog.jetbrains.com/research/2025/12/efficient-context-management/#:~:text=Recently%2C%20several%20studies%20have%20taken,than%20a%20core%20research%20problem). In other words, **balancing context** - feeding the model just the right amount of relevant information - is key to reliable and efficient coding assistance.

To achieve this balance, modern AI agents employ a combination of strategies for context management. **Open-source research** and frameworks have introduced techniques we can adopt in _JidoCode_ to isolate and condense context dynamically. Below we summarize the key approaches (model-agnostic and generally applicable) to managing LLM context, which we will explore in detail:

- **LLM Summarization (Context Compression):** Use the model (or a helper model) to summarize or distill older conversation history and results, preserving essential information in fewer tokens[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=Summarization%3A%20Using%20an%20LLM%20,inserted%20at%20specific%20boundaries%20in). This prevents context overflow by replacing verbose history with concise summaries.
- **Context Pruning/Omission:** Apply heuristics or rules to drop irrelevant or low-value parts of context (e.g. outdated turns, lengthy logs) instead of including everything[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=the%20solution%20is%20to%20cut,Pruning%20is%20essentially%20an). This "observation masking" hides unneeded details while keeping important recent facts[blog.jetbrains.com](https://blog.jetbrains.com/research/2025/12/efficient-context-management/#:~:text=On%20the%20right,logs%20or%20full%20file%20reads).
- **External Memory & Retrieval:** Store important information from the session in an external memory (e.g. vector database or knowledge store) and retrieve only the most relevant pieces when needed[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=world%20state.%20This%20long,repeating%20errors%20and%20to%20improve). This allows effectively **infinite** long-term context without overloading the prompt, as the agent can look up past facts on demand.
- **Sub-Agent Context Isolation:** Divide complex tasks among multiple specialized agents, each with its own focused context window[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=Sometimes%20the%20best%20way%20to,coordinator%20agent%20integrates%20the%20results). Sub-agents work independently on subtasks and report results back to a coordinator agent. This way, each agent deals with a smaller, **isolated context**, and the main agent integrates their summarized outputs[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=facts%2C%20another%20on%20executing%20code%2C,need%20to%20share%20all%20intermediate).

In the sections below, we delve into each of these techniques and discuss how they can be implemented (drawing on open-source examples) to optimize context handling in a multi-session coding assistant.

**Summarization and Context Compression**

One powerful approach is to **summarize** or compress the conversation and state as it grows. Summarization uses an LLM (or sometimes a smaller model) to generate a condensed version of earlier interactions, retaining key points while dropping extraneous detail. Many advanced assistants now automatically summarize old content when nearing context limits. For example, Anthropic's Claude has an _"auto-compact"_ feature that, once a conversation approaches ~95% of the context window, will summarize the conversation history to free up space[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=Summarization%3A%20Using%20an%20LLM%20,inserted%20at%20specific%20boundaries%20in). The summary is then used going forward, allowing the dialogue to continue beyond the raw window limit without forgetting past context. This same idea can be applied in _JidoCode_: when a coding session's chat or log becomes very long, we trigger a summarization of the oldest parts to condense the context.

_Illustration of context management strategies. Left: an agent with a growing raw context (reasoning, actions, and observations from each turn) that eventually exceeds the window. Middle: using_ **_LLM summarization_**_, the agent replaces the verbose history of turns T₁ and T₂ with a compact summary (yellow) that captures the important details_[_blog.jetbrains.com_](https://blog.jetbrains.com/research/2025/12/efficient-context-management/#:~:text=Depicted%20in%20the%20middle%20of,2)_. Right: using_ **_observation masking_**_, the agent retains the past reasoning and decisions but hides the large observation outputs from earlier turns (green), avoiding reprocessing long logs_[_blog.jetbrains.com_](https://blog.jetbrains.com/research/2025/12/efficient-context-management/#:~:text=On%20the%20right,logs%20or%20full%20file%20reads)_._

Summarization offers **practical benefits**: it keeps context size bounded (in theory allowing infinite dialogues via repeated compression)[blog.jetbrains.com](https://blog.jetbrains.com/research/2025/12/efficient-context-management/#:~:text=An%20additional%20difference%20between%20the,also%20allowed%20to%20grow%20infinitely), reduces token processing (speeding up responses and lowering cost), and maintains continuity of important facts. Open-source agent frameworks have embraced this. For instance, **OpenHands** (an open-source coding agent) implements a _context condenser_ that intelligently summarizes older interactions once the conversation exceeds a threshold[openhands.dev](https://openhands.dev/blog/openhands-context-condensensation-for-more-efficient-ai-agents#:~:text=As%20the%20conversation%20grows%20beyond,needing%20to%20retain%20every%20detail). The condenser focuses on the user's goals, the progress made, pending tasks, and critical technical details (like key file names or error messages)[openhands.dev](https://openhands.dev/blog/openhands-context-condensensation-for-more-efficient-ai-agents). By condensing older turns into a concise "memory" of what happened, OpenHands was able to cut per-turn token costs by over half without degrading task performance[openhands.dev](https://openhands.dev/blog/openhands-context-condensensation-for-more-efficient-ai-agents#:~:text=We%20evaluated%20our%20context%20condensation,both%20performance%20and%20efficiency%20metrics)[openhands.dev](https://openhands.dev/blog/openhands-context-condensensation-for-more-efficient-ai-agents#:~:text=Most%20importantly%2C%20these%20efficiency%20gains,solves%20an%20average%20of%2053). We can adopt a similar strategy in _JidoCode_: maintain a running **conversation summary** that gets updated every N turns or when context grows too large. This summary would preserve the essential context (what the user asked, what the assistant has done or decided, important code or outcomes) in a few sentences, which the model sees in the prompt instead of the full verbose history.

**Hierarchical summarization** techniques can be used for very long contexts. Rather than summarizing everything in one go, the conversation or documents can be broken into chunks, summarized individually, and then those summaries are themselves summarized, and so on[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=points%20to%20hand%20off%20a,critical%20info%20while%20dropping%20redundancy). This multi-step compression helps retain important information from each segment in a structured way (for example, summarizing each "phase" of a coding session separately). The _Google ADK_ (Agent Development Kit) platform takes this approach by compacting older events over a sliding window and writing a summary back into the session log as a new event[developers.googleblog.com](https://developers.googleblog.com/architecting-efficient-context-aware-multi-agent-framework-for-production/#:~:text=When%20a%20configurable%20threshold%20,raw%20events%20that%20were%20summarized). Because the summarization is stored in the history (and older raw events can then be pruned), subsequent prompts automatically use the compacted version of the past context[developers.googleblog.com](https://developers.googleblog.com/architecting-efficient-context-aware-multi-agent-framework-for-production/#:~:text=Because%20compaction%20operates%20on%20the,itself%2C%20the%20benefits%20cascade%20downstream). We can design _JidoCode_'s session logger similarly: older interaction records get replaced by a summary entry, ensuring the stored session history remains scalable even over long coding sessions.

It's important to note that summarization is **lossy** - the quality of the summary depends on the model's ability to capture all relevant details. Critical nuances could be lost if the summary is too coarse. If a later task relies on a detail that was dropped, the LLM might produce incorrect or suboptimal results. To mitigate this, we should summarize **carefully** and perhaps conservatively. In practice, one might keep a separate store of original records (outside the prompt) in case a detail needs to be retrieved later, or use very targeted summaries that explicitly preserve things like decisions made, variable names, or error codes that could be needed. Some teams even train dedicated summarizer models or involve humans for high-stakes contexts[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=One%20must%20be%20cautious%3A%20summaries,step%20reliably%20for%20complex%20applications). For our project, an iterative approach makes sense: we can start with automated summarization of older chat logs and test the assistant's performance, ensuring it still recalls necessary details. We might configure the summarization prompt to explicitly list "Open TODOs, decisions made, key error messages, and next steps" to force the inclusion of important information in the summary.

**Context Pruning and Omission (Masking Irrelevant Details)**

An alternative (or complementary) approach to managing context is **pruning** - removing or excluding parts of the context that are deemed irrelevant, rather than rewriting them into a summary. This can be thought of as _observation masking_ or context filtering. The idea is to show the LLM only the information that truly matters for the current query or task, and simply **omit** older or extraneous content. This strategy is simpler than generating summaries and does not risk introducing summarization errors, since we're just dropping content. However, it requires a reliable way to identify which parts of the context are safe to omit.

In a coding assistant scenario, a common pruning heuristic is to **forget the oldest interactions** once the conversation exceeds a certain length. For example, if the assistant has a long history with the user, we might start omitting the earliest exchanges (especially if the project or topic has since shifted). Earlier context is often less relevant than recent context[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=the%20solution%20is%20to%20cut,Provence%2C%20trained%20a%20model%20to). Another effective heuristic is to remove verbose content that the agent generated for its own reasoning or tool outputs, once those have served their purpose. For instance, if the LLM previously read a large file or ran tests and got a long log output, we can drop the full log from subsequent prompts after extracting the important outcome (like "10 tests failed in module X with error Y"). By pruning such verbose observations, we ensure the model isn't reprocessing huge chunks of text on every turn when only the high-level result was needed[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=drop%20the%20oldest%20messages%20once,relevance%20parts).

Open-source AI tools have implemented such masking strategies. The **Cursor** and **Warp** code assistants reportedly use an _"observation masking"_ approach to limit context size[blog.jetbrains.com](https://blog.jetbrains.com/research/2025/12/efficient-context-management/#:~:text=In%20the%20current%20state%20of,the%20simpler%20of%20the%20two). Instead of summarizing everything, they keep the agent's chain-of-thought and prior decisions fully intact, but **hide the environment outputs** (like file contents or error traces) from older turns[blog.jetbrains.com](https://blog.jetbrains.com/research/2025/12/efficient-context-management/#:~:text=On%20the%20right,logs%20or%20full%20file%20reads). This way, the agent still "remembers" what it tried (since its reasoning and actions remain in context), but it doesn't waste tokens and attention on re-reading large outputs that it has already processed. The figure above (right side) illustrated this: prior turns' actions and reasoning are kept, while the large observation from turn T₁ is masked out in turn T₃[blog.jetbrains.com](https://blog.jetbrains.com/research/2025/12/efficient-context-management/#:~:text=On%20the%20right,logs%20or%20full%20file%20reads). In _JidoCode_, we can adopt a similar practice: maintain a structured log of each turn broken into "Action", "Reasoning", and "Observation" parts, and only carry forward the crucial parts. For example, the assistant's thought process (reasoning) and conclusion from a past code execution might be retained as a note, but the raw console output or file dump can be omitted after that turn. This **scopes the context** to what is needed: the agent's decisions and the outcomes, not the full trace of how it got there.

Pruning is essentially a form of **selective context inclusion**. It can be done with simple rules (like drop oldest N messages, or drop any code block over X lines after use). More advanced techniques use _relevance scoring_ to filter context. For example, before each LLM call, we could evaluate which past messages or information are most relevant to the current user query and only include those. Some research even trains models to identify irrelevant context. One paper (nicknamed **Provence**) trained a model to delete irrelevant parts of context for QA tasks[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=Drew%20Breunig%20refers%20to%20this,relevance%20parts). While training a custom model might be overkill for our needs, we can implement a heuristic relevance filter: e.g., only include past interactions if they concern the same file or function currently being discussed, or if they contain an unresolved TODO that is still active. Everything else older than a cutoff can be dropped from the working prompt. This aligns with a principle highlighted by developers at Google: _"scope by default"_ - each sub-task or model call should see the **minimum context required**, and agents should explicitly fetch anything additional they need[developers.googleblog.com](https://developers.googleblog.com/architecting-efficient-context-aware-multi-agent-framework-for-production/#:~:text=processors%2C%20not%20ad,than%20being%20flooded%20by%20default). By defaulting to a minimal context and requiring explicit retrieval for more, we avoid flooding the model with too much information.

One trade-off to note is that simple omission (masking) does **not completely eliminate context growth** - if every turn adds some essential piece (like the agent's decisions), the prompt will still grow slowly over time. Observation masking tends to slow the growth (by cutting out the bulky parts of each turn) but not stop it entirely[blog.jetbrains.com](https://blog.jetbrains.com/research/2025/12/efficient-context-management/#:~:text=An%20additional%20difference%20between%20the,also%20allowed%20to%20grow%20infinitely). In contrast, summarization can periodically compress everything and start fresh with a short summary, allowing effectively unbounded conversations. Therefore, a **hybrid approach** is often best: prune aggressively to remove obvious noise in the short term, and use summarization to condense the accumulating important bits in the longer term. Indeed, the JetBrains research team compared these approaches and even developed a hybrid that combined summarization with selective masking to achieve significant context length efficiency gains[blog.jetbrains.com](https://blog.jetbrains.com/research/2025/12/efficient-context-management/#:~:text=,broader%20application%20of%20our%20study)[blog.jetbrains.com](https://blog.jetbrains.com/research/2025/12/efficient-context-management/#:~:text=,bits%20of%20information%20are%20hidden). _JidoCode_ can similarly use **tiered compression**: for example, always drop irrelevant content each turn (masking), and if the session still grows too large, trigger a summary of the remaining context to condense it further.

**External Memory and Context Retrieval**

Both summarization and pruning deal with the immediate **prompt context** - what text we directly feed into the LLM's context window. Another dimension of context management is leveraging **external memory**: storing information outside the prompt (in a database, vector index, or file system) and retrieving relevant pieces on demand to include when needed. This approach is commonly known as **retrieval-augmented generation (RAG)** when used for knowledge lookup[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=Retrieval), and as a form of long-term memory for conversational agents when used to remember past interactions or facts[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=Long,agent%20reflects%20on%20what%20happened).

In a multi-session coding assistant, external memory can take several forms:

- A **project knowledge base**: For each project, we could index important content (such as documentation, project README, or frequently referenced code files) into a vector store. When the assistant faces a query (e.g., _"How do I use library X in this project?"_), it can embed the query and retrieve the most relevant snippets from the knowledge base to include in the prompt[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=In%20practice%2C%20implementing%20RAG%20involves,model%20is%20asked%20to%20respond)[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=Relevance%20scoring%20is%20the%20mechanism,an%20example%20from%20software%20agents). This ensures the model has the necessary project-specific info at hand without always keeping it in context. Only the **top-K** relevant chunks are added to the prompt, avoiding context pollution with unrelated data[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=Commonly%20this%20is%20done%20via,based%20chunking%2C%20keyword).
- **Long-term conversation memory**: Over the course of a session (or across sessions), the assistant can extract **salient facts** or decisions and store them with embeddings in a vector database[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=world%20state.%20This%20long,repeating%20errors%20and%20to%20improve). For example, after a long refactoring task, the system might store a note like _"Refactored the data access module to use caching; user is concerned about memory usage"_. Later, if a user asks about performance or memory, the assistant can retrieve this fact if it's relevant to the query. This is the idea behind frameworks like _Reflexion_, where after each significant turn the agent generates a brief reflection on what was learned or decided and stores it in an episodic memory buffer[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=extract%20salient%20facts%20from%20each,generated). Impressively, providing an agent with such self-generated memory snippets has been shown to improve performance on tasks (Reflexion's authors achieved higher coding success rates by having the model recall its past mistakes and avoid them)[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=framework%20where%20after%20each%20trial,tuning%2C%20simply%20by).
- **User and session preferences**: We might also maintain a profile of user preferences or context that persists across sessions (e.g., preferred coding style, or the last opened files in a project). Instead of prompting the model with this every time, we store it and only retrieve it when relevant.

The key to using external memory effectively is **relevance filtering** - determining _when_ to inject memory and _which_ memory to inject. We don't want to blindly prepend the entire memory database to every prompt (that would reintroduce the large context problem!). Instead, the system should perform a similarity search or other lookup using the current conversation state as the query, and find the top few relevant memory entries[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=Long,agent%20reflects%20on%20what%20happened)[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=Frameworks%20like%20LlamaIndex%20provide%20ready,and%20the%20context%20window%20may). For example, _JidoCode_ could embed the user's latest question or the current code snippet and query a vector store of past discussions or documentation to fetch closely matching items. Those items can be added into the prompt (perhaps in a section like "Relevant Info:" or as system-provided notes). Libraries like **LlamaIndex** and **LangChain** already provide components to do this: e.g., LlamaIndex's VectorMemory and FactExtractionMemory modules enable storing conversation snippets and facts, and auto-inserting them into the prompt when contextually relevant[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=Frameworks%20like%20LlamaIndex%20provide%20ready,and%20the%20context%20window%20may). We can implement a similar mechanism in Elixir, leveraging an embedding model (like OpenAI's text-embedding or a local alternative) to encode and retrieve memory.

One caution is to avoid retrieving memory that is _loosely_ related or outdated for the current context. If the similarity threshold is too low, the assistant might pull in something that confuses the conversation. A notorious example recounted in the LangChain blog was when ChatGPT's long-term memory feature surfaced an irrelevant past detail (the user's location from an earlier conversation) and injected it into a later response out of context[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=One%20challenge%20with%20long,control%3A%20memory%20retrieval%20should%20be). This startled the user and illustrates how unfiltered memory can feel like a privacy leak or non sequitur. To prevent this, we should ensure memory entries are tagged or filtered by context (e.g., only retrieve project A's memories during sessions on project A), and perhaps require a certain high similarity score or other gating logic before inclusion. The system might also keep track of _time_ or _scope_ - for instance, if the conversation has moved to a new topic or a new day, memories from a very old context might be ignored unless explicitly requested.

In summary, external memory allows an agent to **remember more than fits in the immediate context**, by offloading storage to a database and performing intelligent retrieval. For _JidoCode_, this means we don't have to stuff every past conversation turn or code snippet into the prompt at all times. Instead, we maintain a **knowledge index** of the session/project and use it as needed. This technique is model-agnostic and open-source friendly: numerous implementations (from simple QA bots to advanced agents) use vector similarity search or database lookups to extend an LLM's effective knowledge without requiring special model architecture. We will design our assistant to incorporate a memory retrieval step in its pipeline: before generating a response, it can fetch any highly relevant pieces of context (from documentation, past interactions, or stored facts) and supply them to the model, thereby keeping the prompt focused and concise.

**Sub-Agent Context Isolation in a Multi-Agent Architecture**

When a coding task becomes complex, it can often be **subdivided** - for example, one subtask might involve reading documentation, another might be generating code for a specific function, and another might be running tests. Rather than have a single monolithic agent juggle all this context, a promising approach is to use **multiple specialized agents** (or sub-processes) that handle different aspects of the task, each with a _focused context_. The main (lead) agent can then coordinate these sub-agents and integrate their results[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=Sometimes%20the%20best%20way%20to,coordinator%20agent%20integrates%20the%20results)[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=facts%2C%20another%20on%20executing%20code%2C,need%20to%20share%20all%20intermediate). This **context isolation** via multi-agent design follows the principle of separation of concerns: each sub-agent only needs the context relevant to its specific job, which keeps their prompts small and on-topic, and the lead agent maintains the big picture.

Research by Anthropic provides a compelling case study of this approach. They built a multi-agent research system where a _lead agent_ would spawn several Claude sub-agents in parallel for different subtasks of a query[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=OpenAI%E2%80%99s%20Swarm%20library%20and%20others,by%20distributing%20pieces%20of%20the). For example, if a user's query required investigating multiple aspects of a problem, the lead agent could delegate each aspect to a different worker agent. **Each sub-agent received only the information needed for its subtask** (and instructions/objectives from the lead), and it operated independently, returning a result or summary to the lead agent[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=lead%20agent%20that%20spawns%20specialized,agent%20system%20%28Claude%20orchestrating%20Claude). Because each agent had its own context window, the system as a whole could handle a much larger combined context than any single agent - effectively the context is **distributed** across agents. Anthropic reported that this multi-agent approach solved certain complex queries far better than a single-agent approach, even achieving roughly _90% better performance on a broad research task_ by covering more ground in parallel[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=Anthropic%20reported%20that%20their%20multi,context%20windows%2C%20exploring%20different%20aspects%E2%80%A6). The intuition is that by isolating context, **each agent can deeply focus on one piece of the puzzle without being distracted by others**, and the lead agent can then merge these pieces to form the complete answer.

However, multi-agent context isolation brings challenges in practice. The first is **coordination overhead**. The lead agent needs to manage communication: it must provide each sub-agent with a clear task description and whatever input data they need, then later synthesize their outputs. This coordination itself consumes tokens (the instructions to sub-agents and their results all count toward usage) and can lead to high overall token usage - Anthropic observed their system used ~15× more tokens in total than a single-agent solving the same task sequentially[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=However%2C%20this%20strategy%20comes%20with,subagents%20diverging%20or). So, we gain capability at the cost of efficiency. In _JidoCode_, we should reserve multi-agent approaches for tasks that truly need them (e.g. handling multiple files or parallel tests at once) and be mindful of the token budget.

Another challenge is **consistency and context sharing** between agents. If sub-agents are too isolated, they might duplicate work or contradict each other due to missing context. Anthropic found early on that giving only minimal instructions (like "Agent A: research topic X, Agent B: research topic Y") led to sub-agents misinterpreting the task or doing redundant searches[blog.langchain.com](https://blog.langchain.com/how-and-when-to-build-multi-agent-systems/#:~:text=firsthand%3A). Each sub-agent **needs sufficient context about the overall goal and its role** to avoid diverging from the plan[blog.langchain.com](https://blog.langchain.com/how-and-when-to-build-multi-agent-systems/#:~:text=,fail%20to%20find%20necessary%20information). One approach is to provide each sub-agent with a brief summary of the global context or plan. In other words, _share a ground-truth context_ that anchors all agents. The Cognition Labs team (creators of the "Don't Build Multi-Agents" blog) argue that whenever you spawn a helper agent, you should give it the full action log or conversation state to ensure it has all implicit decisions and facts so far[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=Agents%E2%80%9D%20actually%20argues%20that%20naive,they%20know%20the%20overall%20task). This avoids miscommunication but, of course, **defeats the token savings** of isolation if you literally pass the entire context to every sub-agent. A balanced approach is to share only the **relevant slice of context** with each sub-agent - for example, only the part of the plan or conversation that pertains to that agent's subtask[developers.googleblog.com](https://developers.googleblog.com/architecting-efficient-context-aware-multi-agent-framework-for-production/#:~:text=,slice%20of%20context%20between%20agents). We can achieve this by having the lead agent _summarize or excerpt_ the context for each sub-agent. Indeed, developers note that summarizing the interaction state at hand-off points is useful: one might **summarize the overall progress and open questions** before spawning a new agent, so that the new agent starts with a concise briefing rather than raw lengthy history[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=,maintain%20critical%20info%20while%20dropping). _JidoCode_ could implement a function like spawn_subagent(task, context_summary) where context_summary is a distilled chunk of the main session context relevant to that task. This ensures sub-agents aren't flying blind, while still isolating them from unrelated details.

To illustrate, imagine the user asks the assistant to "Refactor the data parser and then update the documentation accordingly." The main agent could split this into two tasks: (1) refactor code, (2) edit documentation. It could spawn a code-refactor sub-agent with the relevant code file and instructions, and a documentation sub-agent with context about the intended changes. Each sub-agent would work in its own context sandbox (perhaps even using different specialized LLMs or tools), and produce a result: e.g., the refactored code and a summary of changes, and a draft documentation update. The main agent then takes those outputs and integrates them - possibly reviewing or lightly editing - and presents the final results to the user. Throughout this, the sub-agents did not need to know about each other, nor did we clutter their prompts with each other's data. They **only communicated their results back** to the main agent, which decided what to do with them (e.g., incorporate the changes, or maybe ask for further refinement). This aligns with the user's guideline that sub-agents have their own context and only return results to the caller.

A practical pattern to support this is using a **scratchpad** or hidden workspace for sub-agents. For example, Hugging Face's _Deep Researcher_ agent uses an internal scratchpad where the agent's step-by-step reasoning and tool outputs are kept separate from the user-facing conversation[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=A%20compromise%20approach%20is%20using,the%20process%20leaking%20into%20another). The user (and the next turn's prompt) only see the distilled outcome of that reasoning, not the messy interim thoughts. We can mirror this in _JidoCode_: let sub-agents freely generate detailed reasoning or perform actions in an isolated log, but before their result is fed back into the main agent's context, **filter or summarize** it. Perhaps the sub-agent returns a structured result (like "Success: function X refactored; Summary: improved efficiency by Y%; New code: ...") and we only insert the necessary pieces of that into the main conversation (or simply inform the user of success). This prevents irrelevant details or minor errors from one agent's process from leaking into the main context and confusing subsequent steps[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=A%20compromise%20approach%20is%20using,the%20process%20leaking%20into%20another).

To manage multiple agents cleanly, we can draw from open-source orchestration frameworks. **LangChain's LangGraph** (and similar frameworks) emphasize giving developers fine-grained control over what context is passed to each agent and each tool[blog.langchain.com](https://blog.langchain.com/how-and-when-to-build-multi-agent-systems/#:~:text=insight%20has%20guided%20our%20development,context%20engineering%20that%20you%20require). In LangGraph, you explicitly script the sequence of calls and can decide, for instance, that Agent B's prompt consists of only a summary of Agent A's output plus any instructions. There are no hidden global prompts - you manage context hand-offs manually[blog.langchain.com](https://blog.langchain.com/how-and-when-to-build-multi-agent-systems/#:~:text=insight%20has%20guided%20our%20development,context%20engineering%20that%20you%20require). We can implement our own lightweight orchestration in Elixir following this principle: treat the prompt assembly as a pipeline where we choose exactly which pieces of state go into each model invocation. Adopting a **"context compiler"** mindset can help - as Google's ADK team suggests, think of building the prompt as compiling a view from the underlying session state, with explicit transformations (filters, summarizers) applied in sequence[developers.googleblog.com](https://developers.googleblog.com/architecting-efficient-context-aware-multi-agent-framework-for-production/#:~:text=Once%20you%20adopt%20this%20mental,do%20we%20make%20transformations%20observable)[developers.googleblog.com](https://developers.googleblog.com/architecting-efficient-context-aware-multi-agent-framework-for-production/#:~:text=Once%20you%20separate%20storage%20from,maintains%20ordered%20lists%20of%20processors). Each sub-agent call would then be a stage where the context is tailored (compiled) for that specific purpose. By designing _JidoCode_'s architecture around these concepts (clear separation of stored session data vs. the working prompt, and scoped context by default), we ensure the context each agent sees is neither polluted with unnecessary info nor missing crucial details.

**Implementing Context Management in _JidoCode_**

Bringing these ideas together, the _JidoCode_ assistant can incorporate multiple layers of context management to optimize LLM performance:

- **Session Context Isolation:** Each project or session should maintain its own context state, completely separate from other projects. This prevents any cross-talk between projects. In practice, this means we'll identify sessions by IDs and never mix the conversation or memory of session A into session B's prompts. This is a straightforward isolation at the application level.
- **Structured Memory and Logging:** We will log interactions in a structured form (e.g., an event list with fields for user query, assistant answer, tool actions, results, etc.). This will allow us to apply transformations like summarization or filtering on the log easily (rather than dealing with one giant text blob). As Google's architecture suggests, we separate the **durable storage** (full session event log) from the **working context** (what we actually send to the model each turn)[developers.googleblog.com](https://developers.googleblog.com/architecting-efficient-context-aware-multi-agent-framework-for-production/#:~:text=,version%20rather%20than%20pasted%20into)[developers.googleblog.com](https://developers.googleblog.com/architecting-efficient-context-aware-multi-agent-framework-for-production/#:~:text=1,recomputed%20view). The working context is dynamically constructed from the session log for each model invocation, using processors that can select or compress events as needed.
- **Automated Summarization Triggers:** We will implement a mechanism to **auto-summarize** the session history once it grows beyond a certain size (in tokens or number of turns). For example, after every 10 turns, or when the token count of the prompt is > N, the assistant can take the oldest portion of the conversation and summarize it. The summary would then replace or represent those old turns in subsequent prompts. This could be done by having a special "compaction" event in our log (similar to ADK's compaction event[developers.googleblog.com](https://developers.googleblog.com/architecting-efficient-context-aware-multi-agent-framework-for-production/#:~:text=When%20a%20configurable%20threshold%20,raw%20events%20that%20were%20summarized)) that records the summary. The original events might still be kept in storage (for audit or potential retrieval), but marked as _archived_. Our prompt assembly code would then skip archived events and include the summary instead, ensuring the prompt stays within limits and focused.
- **Heuristic Pruning and Masking:** In constructing the prompt each turn, we will apply rules to drop clearly irrelevant content. For instance, we won't include the full content of files that have been read in previous steps - only a snippet or a reference to them ("File X was read"). We will drop detailed tool outputs (like a stack trace) after they have been handled, only keeping a note of the outcome (e.g., "Tests failed for 3 cases"). We'll also prefer to include recent turns over very old ones, unless those old ones contain information that is still needed (which might be determined via the memory retrieval step below). Essentially, each time we build a prompt, we **filter the session events for relevance**: by recency, by topic, and by type of content. This ensures we don't resend noise back to the LLM on every turn.
- **Vector Store Memory for Long-Term Info:** Alongside the transient context, _JidoCode_ can maintain a vector-based memory of important facts, code summaries, or user instructions that persist. Each time a significant event happens (e.g., the user specifies a requirement or the assistant completes a task), we can embed a summary of that and store it. Then, at the start of a new turn, we embed the user's query or the current task description and do a similarity search in the memory. If we find any memory items with high relevance, we inject them into the prompt (e.g., as a "Recall:" section). This way, even if our prompt history was pruned or summarized, we don't lose the ability to recall earlier important points. This approach is widely used in open-source LLM applications to **extend context** - for example, LlamaIndex's VectorMemory and others are exactly for fetching relevant past info on the fly[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=Frameworks%20like%20LlamaIndex%20provide%20ready,and%20the%20context%20window%20may). We'll implement a lightweight version of this using an open-source vector library (FAISS or similar) to index text embeddings.
- **Sub-Agent Orchestration:** When the assistant encounters a complex request that benefits from parallelism or specialized handling, we will spawn sub-agents. Each sub-agent will get a **bespoke prompt** containing: the high-level system instructions (to keep behavior consistent), a brief of the task (from the lead agent), and any specific context it needs (e.g., the particular file content or function relevant to its subtask). The sub-agent will _not_ receive the entire session history - only what the lead agent deems necessary for that task[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=Sometimes%20the%20best%20way%20to,coordinator%20agent%20integrates%20the%20results). As the sub-agent works, it may use tools and produce intermediate reasoning; all of that stays in its own sandbox (perhaps logged for debugging, but not shared upstream unless needed). Once it finishes, it returns a result (which could be a proposed code change, an answer, etc.). The main agent then incorporates that result into the overall context. It might directly present it to the user, or if the task is part of a larger workflow, the main agent could summarize the sub-agent's result and add that summary into its context. This way, the **main session context only grows with distilled results** of sub-tasks, not the full details. If another sub-agent or a later step needs to know what happened in sub-task A, they can be given the summary rather than the raw logs, keeping things clean.
- **Testing and Tuning:** Finally, we will need to test these mechanisms in practice. We'll observe how the LLM's outputs change with different amounts of context. Does summarization maintain enough detail to solve coding problems correctly? Are our pruning rules ever removing something vital? We might find, for example, that our initial summaries omit a variable name that later becomes relevant - in that case, we adjust the summarization prompt to always include variable names or code identifiers. Or we might see that an isolated sub-agent didn't know a piece of info it needed (because we didn't pass it in); then we refine the context handoff to include that info next time. The context management will be an **iterative engineering process**, but thanks to the principles gathered from open-source research, we have a solid starting framework.

In conclusion, managing LLM context in a multi-session coding assistant requires a **mix of strategies**: compressing history through summarization, trimming the fat through omission, extending memory via retrieval, and isolating contexts for concurrent agents. These techniques, developed in various open-source systems and research projects, are largely model-agnostic - they apply to any large language model's prompts. By implementing them in _JidoCode_, we ensure that each model invocation has the right information in focus (and nothing more). This should lead to more relevant code completions, more coherent multi-step reasoning, and efficient use of the context window, ultimately improving the assistant's reliability and performance in complex software development sessions.

**Sources:**

- Jin T. Ruan. "Context Engineering in LLM-Based Agents." _Medium (2024)_ - Techniques for prompt structuring, retrieval augmentation, memory (short-term scratchpads & long-term vector stores), context isolation in multi-agent systems[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=Sometimes%20the%20best%20way%20to,coordinator%20agent%20integrates%20the%20results)[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=facts%2C%20another%20on%20executing%20code%2C,need%20to%20share%20all%20intermediate)[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=A%20compromise%20approach%20is%20using,the%20process%20leaking%20into%20another)[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=Summarization%3A%20Using%20an%20LLM%20,inserted%20at%20specific%20boundaries%20in)[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=points%20to%20hand%20off%20a,critical%20info%20while%20dropping%20redundancy).
- JetBrains Research. "Cutting Through the Noise: Smarter Context Management for LLM-Powered Agents." _JetBrains Research Blog (Dec 2025)_ - Empirical study of context growth in software agent tasks; compares LLM summarization vs. observation masking strategies[blog.jetbrains.com](https://blog.jetbrains.com/research/2025/12/efficient-context-management/#:~:text=On%20the%20right,logs%20or%20full%20file%20reads)[blog.jetbrains.com](https://blog.jetbrains.com/research/2025/12/efficient-context-management/#:~:text=An%20additional%20difference%20between%20the,also%20allowed%20to%20grow%20infinitely).
- OpenHands AI. "OpenHands Context Condensation for More Efficient AI Agents." _OpenHands Blog (Apr 2025)_ - Describes an open-source agent's context condensation feature using summarization of older interactions, achieving reduced token costs without loss of performance[openhands.dev](https://openhands.dev/blog/openhands-context-condensensation-for-more-efficient-ai-agents#:~:text=As%20the%20conversation%20grows%20beyond,needing%20to%20retain%20every%20detail)[openhands.dev](https://openhands.dev/blog/openhands-context-condensensation-for-more-efficient-ai-agents#:~:text=Image%3A%20Condenser%20Functionality%20Overview).
- LangChain. "How and when to build multi-agent systems." _LangChain Blog (Jun 2025)_ - Discusses lessons from Cognition's "Don't Build Multi-Agents" and Anthropic's multi-agent research system; emphasizes context engineering, detailed sub-agent instructions, and careful handoff of context[blog.langchain.com](https://blog.langchain.com/how-and-when-to-build-multi-agent-systems/#:~:text=%3E%20Long,This%20distributed%20approach)[blog.langchain.com](https://blog.langchain.com/how-and-when-to-build-multi-agent-systems/#:~:text=,fail%20to%20find%20necessary%20information)[blog.langchain.com](https://blog.langchain.com/how-and-when-to-build-multi-agent-systems/#:~:text=insight%20has%20guided%20our%20development,context%20engineering%20that%20you%20require).
- Google Developers. "Architecting efficient context-aware multi-agent frameworks for production." _Google Developers Blog (2023)_ - Introduces principles like separating storage vs prompt view, explicit context processing pipelines, scoping context by default, and a tiered approach to session memory with automatic compaction (summarization) events[developers.googleblog.com](https://developers.googleblog.com/architecting-efficient-context-aware-multi-agent-framework-for-production/#:~:text=processors%2C%20not%20ad,than%20being%20flooded%20by%20default)[developers.googleblog.com](https://developers.googleblog.com/architecting-efficient-context-aware-multi-agent-framework-for-production/#:~:text=When%20a%20configurable%20threshold%20,raw%20events%20that%20were%20summarized).
- LlamaIndex Documentation - Descriptions of memory modules (VectorMemory, etc.) and workflows for retrieving relevant context from indexes[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=Frameworks%20like%20LlamaIndex%20provide%20ready,and%20the%20context%20window%20may).
- Shinn et al. "Reflexion: an autonomous agent with dynamic memory." _ArXiv preprint (2023)_ - Demonstrates improved coding task performance by having the agent summarize and store self-reflections, which are retrieved to avoid repeating mistakes[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=extract%20salient%20facts%20from%20each,generated).
- Park et al. "Generative Agents: Interactive Simulacra of Human Behavior." _ArXiv (2023)_ - Showcases long-term memory in simulated agents via recording events and periodic summarization into higher-level reflections[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=Another%20influential%20work%20is%20Generative,agents%20to%20remember%20past%20events)[jtanruan.medium.com](https://jtanruan.medium.com/context-engineering-in-llm-based-agents-d670d6b439bc#:~:text=agent%20recorded%20every%20observation%20and,The%20researchers) (applied in a different context, but inspires memory design).
