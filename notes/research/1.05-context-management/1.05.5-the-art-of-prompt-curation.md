





























# Navigating the Labyrinth: A Deep Dive into Context Management for Large Language Models

## The Expanding Context Frontier: Opportunities and Inherent Challenges

The rapid evolution of Large Language Models (LLMs) has been characterized by an almost insatiable appetite for larger context windows. From the initial capabilities of models handling a few hundred tokens, we have now entered an era where systems can process hundreds of thousands, even millions, of tokens in a single inference pass. This expansion is not merely a technical milestone; it fundamentally alters the landscape of what is possible with AI, promising to transform applications from multi-document analysis and long-form code generation to intricate, multi-turn conversational agents that can maintain coherence over extended interactions. The Jido agentic framework, with its AI extension, JidoAI, aims to operate precisely within this domain of complex, multi-step reasoning and action. For such a system, the ability to effectively manage and utilize vast amounts of contextual information—from user prompts and agent memory to retrieved documents and tool outputs—is not just an enhancement but a core prerequisite for advanced functionality. However, this new frontier of expansive context is fraught with challenges that are as profound as the opportunities it presents. The naive assumption that a larger context window uniformly translates to better performance is a dangerous misconception. As research is increasingly revealing, the effective utilization of long context is a nuanced problem, riddled with computational bottlenecks, emergent model behaviors, and a fundamental tension between quantity and quality of information. The journey into effective context management is, therefore, an exploration into the very heart of how LLMs perceive, process, and prioritize information, demanding a sophisticated toolkit that goes far beyond simply feeding a model more data.

The primary challenges associated with long contexts can be broadly categorized into two interrelated domains: computational and cognitive. The computational challenges are the most immediately apparent. The self-attention mechanism, the powerhouse of Transformer-based LLMs, suffers from quadratic complexity in both time and memory with respect to the sequence length. This means that doubling the context length quadruples the computational resources required for attention calculations. For an agent like Jido, which may need to perform rapid, successive inferences to reason and act, this can lead to prohibitive latency and operational costs, rendering real-time interaction unfeasible. The memory footprint for caching the Key and Value (KV) states of every token in the context also grows linearly with sequence length, quickly exhausting available GPU memory and limiting the batch size that can be processed efficiently. These hard physical constraints necessitate strategies that can mitigate the computational burden without critically compromising the model's access to necessary information. Techniques such as window attention, which only retains a subset of recent KV states, offer a direct solution but, as research has shown, can introduce their own set of problems, leading to performance degradation when the model encounters sequences longer than its predefined window [[11](https://arxiv.org/abs/2309.17453)]. This brings us to the more subtle and perhaps more fascinating "cognitive" challenges: how LLMs actually use the context they are given.

The cognitive challenges revolve around the model's ability to effectively retrieve and reason with information within a vast input space. Counterintuitively, more information does not always lead to better outcomes. The seminal paper "Lost in the Middle: How Language Models Use Long Contexts" provides compelling evidence of this [[3](https://arxiv.org/abs/2307.03172)]. Through rigorous experiments on multi-document question answering and key-value retrieval tasks, the authors demonstrated that the performance of current language models is highly sensitive to the position of relevant information within the context window. Instead of uniformly accessing information across the entire input, models exhibit a strong positional bias, performing best when crucial data is located at the very beginning or the very end of the context. Performance degrades significantly, sometimes dramatically, when the model must access information situated in the middle of a long context. This "lost in the middle" phenomenon reveals a critical vulnerability: even if an agent system successfully retrieves and includes all relevant documents in a prompt, the LLM may still fail to use that information effectively simply due to its placement. This finding has profound implications for the design of AI agents. It suggests that a naive concatenation of retrieved documents, chat history, and other contextual data into a single, flat prompt is a suboptimal strategy. Instead, a sophisticated context management system must be aware of these model biases and employ intelligent strategies—such as strategically reordering information or highlighting key passages—to guide the model's attention towards what truly matters. This is not just about filtering noise; it's about intelligently curating and presenting the signal in a way that aligns with the model's inherent processing tendencies.

The path forward, therefore, is not to simply seek ever-larger context windows, but to develop intelligent, adaptive, and efficient context management strategies. These strategies must form a multi-layered defense against the twin problems of computational intractability and cognitive overload. They can be conceptualized as a spectrum, ranging from methods that optimize the existing attention mechanisms, to those that fundamentally restructure how information is presented, to those that offload memory entirely from the model's immediate context. At one end of this spectrum lie **Attention Optimization** techniques like StreamingLLM, which directly tackle the KV cache problem by identifying and preserving crucial "attention sinks" to maintain stability in infinite-length streams [[11](https://arxiv.org/abs/2309.17453)]. In the middle, we find **Information Restructuring** methods, such as those inspired by "Lost in the Middle," which focus on curating the input prompt to maximize model performance by strategically placing the most salient information [[3](https://arxiv.org/abs/2307.03172)]. This also includes summarization and prompt compression techniques that aim to distill the essence of long documents into more digestible formats. Further along the spectrum are **Architectural Innovations** like RecurrentGPT, which proposes a paradigm shift by using natural language itself as a memory, simulating a recurrence mechanism that allows for the generation of arbitrarily long text without being constrained by a fixed context window [[10](https://arxiv.org/abs/2305.13304)]. Finally, there is the paradigm of **Retrieval-Augmented Generation (RAG)**, an external memory approach where the LLM's role is decoupled from knowledge storage. Instead of embedding all information into the context, a retrieval system fetches relevant knowledge on-demand from a vast external database, providing a highly scalable and efficient solution, though one that introduces its own complexities around retrieval accuracy and integration. For the AshAi package within the Jido framework, the goal is not to choose a single winner from these approaches but to create a flexible, configurable system that can leverage the right combination of these strategies, adapting dynamically to the specific demands of each task, the available computational resources, and the evolving capabilities of the underlying LLMs.

## The Architecture of Focus: Core Strategies for Efficient Context Utilization

The challenge of managing extensive contexts for Large Language Models has catalyzed a diverse array of research, each proposing unique architectural and algorithmic solutions. These strategies are not mutually exclusive but rather represent different points of leverage in the complex system of LLM inference. For a framework like Jido, which requires both deep reasoning and efficient action, understanding the nuances of these approaches is critical for building a robust AI extension. The core strategies can be broadly classified into three categories: optimizing the existing attention mechanism to handle longer sequences more efficiently, restructuring the information presented to the model to align with its cognitive biases, and fundamentally augmenting the model's architecture with external memory systems. Each of these pathways offers a distinct set of trade-offs between performance, computational cost, and implementation complexity, and together they form the bedrock upon which a comprehensive context management solution like AshAi can be built. The journey begins with a deep dive into these foundational strategies, exploring their mechanisms, their strengths, and their inherent limitations, all while keeping an eye on their practical applicability within an Elixir-based ecosystem.

### Optimizing Attention: The Mechanics of Focus

The self-attention mechanism is both the greatest strength and the most significant bottleneck of modern Transformer-based LLMs. Its ability to weigh the relevance of every token against every other token in the context is what gives these models their powerful contextual understanding. However, this very capability leads to the quadratic scaling of computation and memory, making the processing of very long sequences infeasible under standard approaches. The research community has responded with ingenious methods designed to optimize attention, making it more efficient and scalable without fundamentally abandoning its core principles. One of the most impactful recent developments in this area is StreamingLLM, a framework designed to enable LLMs, trained on finite-length sequences, to generalize to streaming, infinite-length contexts without any fine-tuning [[11](https://arxiv.org/abs/2309.17453)]. StreamingLLM directly addresses the two primary challenges of streaming applications: the unbounded growth of the KV cache and the failure of models to generalize to sequences longer than those seen during training. A naive solution, window attention—where only the KV states of the most recent K tokens are cached—fails catastrophically when the text length surpasses this cache size, leading to a rapid deterioration in performance and model stability. The key insight of the StreamingLLM paper is the discovery of the "attention sink" phenomenon. Through careful analysis, the researchers observed that the initial tokens in a sequence receive disproportionately high attention scores, acting as a "sink" for attention, even when they are not semantically critical. These initial tokens serve to stabilize the attention distribution, and their removal is what causes the failure of simple window attention.

StreamingLLM leverages this discovery by proposing a simple yet highly effective caching strategy: instead of discarding all tokens outside the recent window, it retains both the initial tokens (the attention sinks) and the most recent K tokens (the sliding window). This approach, termed "Sink + Window" attention, allows the model to maintain stable performance over sequences of arbitrary length. The framework demonstrates remarkable success, enabling models like Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens. The performance gains are substantial, with StreamingLLM showing up to a 22.2x speedup compared to a sliding window recomputation baseline in streaming settings [[11](https://arxiv.org/abs/2309.17453)]. The implications for an agentic framework like Jido are profound. Many agent tasks, such as multi-round dialogues, code debugging sessions, or interactive data analysis, are inherently streaming in nature. The agent needs to maintain a coherent "memory" of the interaction over time. StreamingLLM provides a practical and efficient mechanism for this, allowing Jido to manage conversational state and long-term context without constantly reprocessing the entire history or running into memory errors. Furthermore, the paper suggests that the performance can be further improved by adding a dedicated placeholder token as an attention sink during pre-training. While this is a consideration for model developers, for a framework like Jido that uses existing models, the ability to apply StreamingLLM without any fine-tuning is a significant advantage, making it a prime candidate for integration into the AshAi context management toolkit.

Another approach to managing context, though not directly explored in the provided data but conceptually related to information management, is context pruning or filtering. The core idea is to identify and remove less relevant portions of the context before it is passed to the LLM. This can be seen as a pre-processing step that complements attention optimization techniques. While the "H2O" paper was not directly accessible, its title, "H2O: Heavy-tail Regularization for Over-parameterized Language Models," suggests a mechanism for identifying and managing the influence of less important parameters or, by extension, less important tokens. Such techniques aim to make the context denser with relevant information, reducing noise and allowing the model to focus its capacity on what matters most. When combined with StreamingLLM, one could imagine a system where a larger context is first intelligently pruned of redundant or irrelevant information, and then the remaining, more salient tokens are processed using the efficient "Sink + Window" attention mechanism. This layered approach would offer both efficiency and focus, ensuring that the LLM's limited computational resources are directed towards the most valuable parts of the input. The development of such pruning strategies requires robust methods for measuring token or segment relevance, which could range from simple TF-IDF scores to more sophisticated gradient-based or model-based importance estimators. The challenge lies in making this pruning process fast and accurate enough to be a net benefit, avoiding a scenario where the computational cost of pruning outweighs the gains from a smaller context.

### Restructuring Information: The Art of Prompt Curation

While optimizing the attention mechanism tackles the problem from the inside out, another powerful line of attack focuses on restructuring the information *before* it is fed to the model. This approach acknowledges that LLMs have specific, and sometimes surprising, biases in how they process input, and that intelligent prompt curation can be as effective, if not more so, than complex architectural changes. The most striking evidence for this comes from the "Lost in the Middle" study, which meticulously documented how the position of relevant information within a context window dramatically impacts an LLM's ability to use it [[3](https://arxiv.org/abs/2307.03172)]. This research, conducted on models like GPT-3.5 and Claude, revealed a U-shaped performance curve: accuracy is highest when crucial information is located at the beginning or the end of the input context, and it dips significantly when that information is placed in the middle. This finding is critical because it directly challenges the common practice of simply concatenating retrieved documents or conversation history in chronological order. For an AI agent that relies on retrieving information from various sources, this means that the retrieval step is only half the battle; the presentation of that information is equally important. The "Lost in the Middle" paper, therefore, provides a clear, actionable guideline for context management: prioritize the placement of the most relevant information at the extremities of the context window.

For the AshAi package, this insight translates into a "Prompt Ordering" or "Information Prioritization" module. When multiple pieces of retrieved information, such as document snippets or past conversational turns, need to be presented to the LLM, this module would not just concatenate them. Instead, it would employ a ranking strategy to identify the most salient pieces of information and place them strategically at the beginning or end of the prompt. The middle portion of the context could then be filled with less critical, supporting, or background information. This simple reordering, based on a deep understanding of model behavior, can yield significant performance improvements without any changes to the underlying LLM architecture. The ranking mechanism itself could be multifaceted, considering factors such as the relevance score from the retrieval system, the recency of the information, or even explicit user cues. This strategy turns the context window from a passive bag of information into an actively curated structure designed to guide the model's attention where it's needed most. It's a form of "prompt engineering" elevated to a systematic, algorithmic level, perfectly suited for integration into a framework like Jido where reliability and performance are paramount.

Beyond reordering, another critical information restructuring technique is summarization. When dealing with very long documents or extensive conversation histories, it is often impractical to include the full text. Summarization aims to compress this information into a shorter, more concise form while preserving its core meaning and relevance. This can be applied in various ways within an agentic workflow. For instance, older parts of a conversation could be periodically summarized into a "state summary," which is then passed along with more recent, verbatim turns. Similarly, long documents retrieved by an agent could be first summarized by a smaller, faster LLM (or even by the same LLM in a separate step) to produce a condensed version that fits more easily within the context window alongside other information. The trade-off with summarization is the potential loss of detail. A poorly crafted summary might omit the very piece of information crucial for the task. Therefore, effective summarization strategies often need to be query-aware or "abstractive," focusing on preserving information relevant to the agent's current goal rather than just providing a generic overview. This might involve iterative summarization, where the LLM is prompted to extract and summarize specific facts, or the use of techniques that preserve key entities and relationships from the original text. Combining summarization with the ordering principles from "Lost in the Middle" could prove particularly powerful: the most important summaries could be placed at the prime real estate at the start or end of the context, while more detailed, but less critical, full-text snippets could be relegated to the middle.

The paper "RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text" offers a more radical approach to information restructuring by proposing a fundamental change in how LLMs interact with context over time [[10](https://arxiv.org/abs/2305.13304)]. Instead of trying to fit an ever-expanding context into a fixed-size window, RecurrentGPT simulates the recurrence mechanism of Recurrent Neural Networks (RNNs) using natural language. At each timestep, the LLM (e.g., ChatGPT) generates a paragraph of text and then explicitly updates two types of "memory" stored in natural language: a short-term memory (akin to the prompt) and a long-term memory stored on an external hard drive. This long-term memory consists of a summary of events and key points from previous "timesteps." This approach effectively decouples the text generation process from the fixed context limit of the underlying LLM, enabling the generation of arbitrarily long texts without the model "forgetting" what happened earlier. The interpretable nature of this language-based memory is another significant advantage; human users can easily read and edit these memories, making the system transparent and enabling interactive long-form generation. For an agent like Jido, this concept is highly intriguing. It suggests a model for agent memory that is persistent, inspectable, and manageable. The agent's "state" wouldn't be an opaque, monolithic context blob, but a structured set of natural language memories that could be queried, updated, and pruned over time. This aligns well with Elixir's strengths in building stateful, concurrent systems, where an agent's state could be a GenServer holding these memory structures. While RecurrentGPT presents a novel way to generate long text, its core principle of using an external, language-based memory to overcome context limitations is broadly applicable to any AI system that needs to maintain long-term coherence and state.

### Augmenting Architecture: The External Memory Paradigm

The strategies discussed so far primarily focus on optimizing how the LLM uses its fixed, internal context window. However, a fundamentally different paradigm exists: instead of trying to cram more information into the context, why not give the model access to an external memory bank that it can consult as needed? This is the core idea behind Retrieval-Augmented Generation (RAG), a hybrid approach that combines the strengths of parametric memory (the knowledge encoded in the LLM's weights) with non-parametric memory (a vast, external database of information). In a typical RAG system, when a user query is received, it is first used to retrieve relevant documents or passages from an external corpus, such as a vector database. These retrieved passages are then injected into the LLM's context window, along with the original query, to guide the generation of the final answer. This architecture elegantly sidesteps the context length limitation for knowledge-intensive tasks, as the LLM doesn't need to store all the world's knowledge internally; it just needs to know how to find and use it. For the Jido framework, which is designed to interact with external tools and data sources, RAG is a natural and essential component. It allows the agent to ground its responses in specific, up-to-date information retrieved from documents, APIs, or other knowledge bases provided by the user.

The benefits of RAG are manifold. Firstly, it greatly enhances the factual accuracy and relevance of the LLM's output by anchoring it to specific, verifiable sources. This reduces the tendency for models to "hallucinate" or make up facts. Secondly, it allows for dynamic knowledge updates; to incorporate new information, one only needs to update the external database, without the need for expensive and time-consuming model retraining. Thirdly, it provides a degree of explainability, as the system can cite the sources it used to generate an answer, allowing users to verify the information. However, RAG also introduces its own set of complexities and challenges. The performance of a RAG system is heavily dependent on the quality of its retrieval component. If the retrieval system fails to find relevant documents, or if it retrieves irrelevant or noisy information, the LLM's performance will suffer, a phenomenon often referred to as "garbage in, garbage out." The design of the retrieval mechanism—how documents are indexed, how queries are formulated, and how relevance is scored—is therefore critical. Furthermore, there is the challenge of effectively integrating the retrieved information into the prompt. How many documents should be retrieved? How should they be formatted? And, crucially, how should they be ordered? This is where the insights from "Lost in the Middle" become directly applicable: even within a RAG framework, the placement of the retrieved snippets in the final prompt can significantly impact the LLM's ability to synthesize an answer. A sophisticated AshAi context manager would therefore not only handle the retrieval logic but also the intelligent presentation of the retrieved results, potentially combining them with summarization if the retrieved texts are too long, and ordering them based on relevance and the LLM's positional biases.

The concept of external memory can be taken even further, as hinted at by the RecurrentGPT paper [[10](https://arxiv.org/abs/2305.13304)]. While RAG typically retrieves read-only information, a more advanced agent could possess a read-write memory that it can update over time. This memory could store facts learned from interactions, user preferences, or intermediate results from long-running tasks. This creates a more personalized and stateful agent that can learn and adapt. In an Elixir environment like Jido, this is a particularly compelling model. An agent's memory could be implemented as an Elixir GenServer or using a more robust data store like ETS or even a persistent database like PostgreSQL. The agent's workflow would then involve a continuous loop of perceiving the environment (user input, tool outputs), reasoning (using the LLM with current context and memory), acting (calling tools, generating responses), and then updating its memory based on the outcome. This aligns perfectly with the actor model and the fault-tolerant, concurrent nature of the BEAM VM, allowing each agent instance to maintain its own isolated, persistent state. The challenge in this paradigm lies in managing the memory itself: preventing it from growing without bound, ensuring its consistency, and developing efficient mechanisms for the LLM to query and update it. This might involve dedicated memory management modules that can summarize, compress, or forget old information, and well-defined protocols for the LLM to interact with its memory store. This vision of an LLM with a sophisticated, external, read-write memory represents a significant step towards more capable and autonomous AI agents.

## Synthesizing a Context-Aware Future: Implications for the AshAi Package

The exploration of context management strategies reveals a clear mandate: the future of capable and efficient Large Language Models lies not in the brute-force expansion of context windows alone, but in the development of intelligent, multi-layered systems that understand how to curate, prioritize, and utilize information. For the AshAi package within the Jido agentic framework, this research provides a rich blueprint for building a context management system that is both powerful and pragmatic. The goal is not to implement a single, monolithic solution, but to create a flexible and configurable toolkit that can adapt to a wide variety of tasks and computational constraints. The insights from papers like "Lost in the Middle," "StreamingLLM," and "RecurrentGPT" are not abstract academic concepts; they are actionable guidelines that can be translated into robust Elixir code, leveraging the unique strengths of the BEAM ecosystem to build a next-generation AI extension. The path forward involves synthesizing these diverse strategies into a cohesive architecture that empowers developers to control how their agents interact with information, ensuring that Jido remains responsive, accurate, and resource-efficient even when dealing with complex, long-horizon problems.

The design philosophy for AshAi's context management should be one of **composable strategies**. Different tasks demand different approaches. A simple, short-form question-answering bot might only need basic RAG, while a long-running, multi-turn research assistant will require a more sophisticated combination of streaming conversation management, periodic summarization, and an external memory store. Therefore, AshAi should expose a set of configurable context "handlers" or "managers" that can be selected and combined. For instance, a developer could configure a `Jido.Agent` with a context manager that uses StreamingLLM for handling the conversational history, a RAG component for retrieving documents from a user-provided knowledge base, and an information ordering module that prioritizes the most recent user messages and the highest-scoring retrieved documents at the beginning and end of the final prompt, as suggested by the "Lost in the Middle" findings [[3](https://arxiv.org/abs/2307.03172)]. This modular approach allows for flexibility and future-proofing; as new research emerges, new handlers can be developed and plugged into the system without requiring a complete rewrite. The configuration could be specified in the agent's definition, perhaps using a DSL that allows developers to declaratively define their context pipeline, for example: `context_manager: [StreamingLLM.new(window_size: 4096, num_sinks: 4), RAG.new(retriever: MyRetriever, top_k: 5), OrderByRelevance.new()]`. This makes the system transparent and easy to adapt for specific use cases.

The implementation of these strategies in Elixir presents a unique opportunity. The Erlang Virtual Machine (BEAM), on which Elixir runs, is renowned for its lightweight concurrency, fault tolerance, and distributed nature. These features map beautifully onto the requirements of an advanced AI agent. An agent's state, including its context, memory, and KV caches, can be encapsulated within an Elixir `GenServer` or `Agent` process. This provides a natural way to manage the agent's lifecycle, ensuring that its state is isolated and can be updated concurrently. For example, the KV cache required by StreamingLLM [[11](https://arxiv.org/abs/2309.17453)] could be managed within a dedicated process, or even more efficiently, by leveraging an `Nx`-backed binary placed in shared memory or on the GPU, with the Elixir process holding a reference and managing the logic for eviction and updates. For more persistent memory, as envisioned by RecurrentGPT [[10](https://arxiv.org/abs/2305.13304)], Elixir's built-in `:ets` (Erlang Term Storage) tables offer a fast, in-memory solution for storing an agent's long-term memories, key-value pairs, or summarized conversation states. For durability and scalability, this can be extended to integrate with databases like Mnesia or PostgreSQL, allowing agent state to persist across application restarts and be shared across a cluster of nodes. The asynchronous nature of Elixir, powered by message passing, is ideal for orchestrating the different stages of a complex context management pipeline. A user's incoming message can trigger an asynchronous workflow: a retrieval task is spawned to query a vector database, concurrently, a summarization task might be triggered on old conversation history, and once all these asynchronous jobs complete, their results are collected, ordered, and assembled into the final prompt for the LLM. This non-blocking approach ensures that the Jido framework remains responsive, even when performing computationally intensive context management tasks.

Looking beyond the immediate implementation, integrating these context management strategies opens the door to more advanced and autonomous forms of agency. An agent that can effectively manage its own context and memory is a foundational step towards meta-cognition and self-improvement. For instance, an agent could be designed to monitor its own performance. If it notices that its answers are consistently being flagged as incorrect or unhelpful, it could hypothesize that its context is insufficient or noisy. It could then autonomously adjust its own parameters, such as increasing the `top_k` value in its RAG retriever, changing the summarization strategy for its long-term memory, or even explicitly asking the user for clarification or more information. This creates a feedback loop where the agent learns to manage its context more effectively over time. Furthermore, the explicit, language-based memory proposed by RecurrentGPT [[10](https://arxiv.org/abs/2305.13304)] could be used not just for storage but for reasoning. The agent could perform "self-talk," writing down its thoughts, intermediate conclusions, and plans into its memory. This externalizes the reasoning process, making it more robust and inspectable, and allows the agent to perform multi-step reasoning that would otherwise exceed its context window. The Jido framework, with its action-oriented design, is perfectly positioned to exploit this. Actions could include not just external API calls but also internal "cognitive" actions like "summarize_memory," "retrieve_relevant_facts," or "formulate_plan," all of which interact with the agent's managed context and memory stores.

In conclusion, the research into LLM context management is not just an academic pursuit but a practical guide to building more capable and reliable AI systems. The journey from the quadratic limitations of standard attention to the efficient streaming of millions of tokens, from the naive concatenation of prompts to the intelligent, bias-aware curation of information, and from the fixed internal memory of a model to the expansive, persistent, and interactive external memory of an agent, marks a significant evolution in our understanding of how to harness the power of LLMs. For the AshAi package and the Jido framework, these insights are the building blocks of a truly intelligent context management system. By embracing a modular, composable, and Elixir-native implementation, we can create an AI extension that is not only powerful and efficient but also a joy for developers to work with. The future of AI agents lies in their ability to navigate the vast labyrinth of information with purpose and focus, and by embedding these research-backed principles into its core, AshAi can ensure that Jido is at the forefront of this exciting frontier.

# References

[3] Lost in the Middle: How Language Models Use Long Contexts. https://arxiv.org/abs/2307.03172.

[10] RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text. https://arxiv.org/abs/2305.13304.

[11] Efficient Streaming Language Models with Attention Sinks. https://arxiv.org/abs/2309.17453.
