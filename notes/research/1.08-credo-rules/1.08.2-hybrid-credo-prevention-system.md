# Hybrid Credo Prevention System for JidoCode

Preventing Credo violations during LLM code generation is significantly more effective than post-hoc detection. This report provides a complete system design combining **system prompt rules** (targeting 70% prevention), **ontology-based contextual guidance** (ensuring project consistency), and **iterative feedback loops** (catching remaining issues). Research from Cursor, Aider, Claude Code, and academic papers confirms that this layered approach dramatically reduces code review cycles and developer friction.

## The three-layer prevention architecture

The hybrid system operates across three synchronized layers, each addressing a different class of violations:

| Layer | Timing | Target | Expected Impact |
|-------|--------|--------|-----------------|
| **System Prompt Rules** | Pre-generation | Common violations, style conventions | ~70% prevention |
| **Ontology/RAG Context** | Pre-generation | Project-specific patterns, consistency | ~20% prevention |
| **Feedback Loop** | Post-generation | Edge cases, complex violations | Catches remaining ~10% |

Research from Microsoft's LLMLOOP framework and Qodo's compiler-in-the-loop studies shows that **3-5 iterations** in the feedback loop achieve diminishing returns, making the pre-generation layers critical for efficiency.

---

## System prompt Credo style guide

The following condensed guide is optimized for LLM system prompts. At **~600 words**, it balances coverage with context window efficiency. Research from Cursor's `.mdc` rules system shows prompts under 500 lines perform best.

```markdown
## Elixir/Credo Style Requirements

### Naming (snake_case is mandatory)
- Functions/variables/attributes: `get_user`, `user_id`, `@max_retries`
- Modules: PascalCase with uppercase acronyms: `HTTPClient`, `XMLParser`
- Predicates: `valid?` for functions, `is_valid` prefix for guard macros
- Exceptions: consistent suffix, prefer "Error": `InvalidHeaderError`

### Documentation (required for public APIs)
- Every public module needs `@moduledoc` or `@moduledoc false`
- Every public function needs `@doc` before `@spec`
- Add `@spec` type specifications for all public functions
- Empty line after `@moduledoc`, no empty line between `@doc` and function

### Code structure limits
- Line length: max **120 characters**
- Cyclomatic complexity: max **9** per function
- ABC size (Assignments+Branches+Conditions): max **30**
- Function arity: max **8** parameters
- Nesting depth: max **2** levels (if/case/cond)

### Pipe chain rules (frequently violated)
- Start pipes with raw values or variables, NOT function calls:
  ```elixir
  # CORRECT
  user_id
  |> fetch_user()
  |> format_response()
  
  # WRONG - starts with function call
  fetch_user(user_id)
  |> format_response()
  ```
- Single pipes discouraged; use direct call: `format(fetch(id))`
- Never start with `Enum.map(list, fn...)`—assign list first

### Control flow (avoid these anti-patterns)
- Never `unless` with `else`—use `if` instead
- Never `unless` with negation: `unless !x`—use `if x`
- Never `if !condition` with else—flip the branches
- `cond` needs 3+ branches; use `if/else` for 2 branches
- Prefer pattern matching over conditionals where possible

### Warnings (production blockers)
- No `IO.inspect`—use `Logger.debug/info/warn/error`
- No `IEx.pry` or `dbg()` calls
- No unused variables (prefix with `_` if intentional)
- No unused return values from Enum/String/Map operations
- Avoid `Mix.env()` at compile time in library code

### Module structure order
1. `@moduledoc`
2. `@behaviour` declarations
3. `use` statements
4. `import` statements
5. `alias` statements (group related aliases)
6. `require` statements
7. Module attributes (`@type`, `@callback`)
8. Struct definition
9. Public functions
10. Private functions

### Formatting details
- 2-space indentation (soft tabs, no hard tabs)
- Spaces around operators and after commas
- No trailing whitespace
- Single blank line between function groups
- No space after `!` negation
- Parentheses for function definitions with params
- Large numbers use underscores: `1_000_000`
```

### Prompt integration strategy

Research from Cursor and GitHub Copilot shows these integration patterns work best:

```
You are an Elixir coding assistant. Before outputting any code:
1. Mentally run "credo --strict" validation
2. Ensure all public functions have @doc and @spec
3. Verify pipe chains start with values, not function calls
4. Check line length stays under 120 characters

[Insert condensed style guide above]
```

---

## Ontology-based contextual guidance

The semantic ontology enables **project-specific pattern extraction** before code generation. This layer ensures generated code matches existing conventions rather than just following generic rules.

### SPARQL queries for pattern extraction

**Query 1: Extract function naming patterns from similar modules**

```sparql
PREFIX code: <http://jidocode.org/ontology/>
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>

SELECT ?prefix (COUNT(?function) AS ?usage_count)
WHERE {
  ?module rdf:type code:Module ;
          code:hasFunction ?function .
  ?function code:functionName ?name .
  BIND(REPLACE(?name, "^([a-z]+)_.*", "$1") AS ?prefix)
}
GROUP BY ?prefix
ORDER BY DESC(?usage_count)
LIMIT 20
```

**Query 2: Find function signatures in target module**

```sparql
PREFIX code: <http://jidocode.org/ontology/>

SELECT ?functionName ?spec ?params ?visibility
WHERE {
  ?module rdf:type code:Module ;
          code:moduleName "MyApp.UserService" ;
          code:hasFunction ?function .
  ?function code:functionName ?functionName ;
            code:visibility ?visibility .
  OPTIONAL { ?function code:hasSpec ?spec }
  OPTIONAL { ?function code:parameters ?params }
}
ORDER BY ?functionName
```

**Query 3: Extract import/alias patterns from similar modules**

```sparql
PREFIX code: <http://jidocode.org/ontology/>

SELECT ?module ?aliasName ?aliasOf
WHERE {
  ?module rdf:type code:Module ;
          code:moduleName ?moduleName ;
          code:hasAlias ?alias .
  ?alias code:aliasName ?aliasName ;
         code:aliasOf ?aliasOf .
  FILTER(CONTAINS(?moduleName, "Controller"))
}
```

**Query 4: Find structural patterns for GenServer modules**

```sparql
PREFIX code: <http://jidocode.org/ontology/>

SELECT ?moduleName 
       (COUNT(DISTINCT ?publicFn) AS ?public_count)
       (COUNT(DISTINCT ?callback) AS ?callback_count)
WHERE {
  ?module rdf:type code:Module ;
          code:moduleName ?moduleName ;
          code:usesModule code:GenServer .
  OPTIONAL { 
    ?module code:hasFunction ?publicFn . 
    ?publicFn code:visibility "public" 
  }
  OPTIONAL { ?module code:hasCallback ?callback }
}
GROUP BY ?moduleName
```

### Context injection prompt template

```markdown
## Project Context (extracted from codebase ontology)

### Module Structure Pattern
This module follows the GenServer pattern used in similar modules:
- Uses: GenServer, Logger
- Common aliases: {extracted_aliases}
- Function naming prefixes: get_, fetch_, handle_, do_

### Similar Functions (few-shot examples)
```elixir
@doc "Fetches user by ID from the database."
@spec get_user(integer()) :: {:ok, User.t()} | {:error, :not_found}
def get_user(user_id) when is_integer(user_id) do
  # ... implementation
end
```

### Task
Generate a function with:
- Name: {function_name}
- Purpose: {description}
- Follow the patterns shown above exactly
```

Research from Qodo AI shows that **3-5 few-shot examples** are optimal—more examples show diminishing returns and consume context window.

---

## Error feedback loop implementation

Research across Cursor, Aider, Claude Code, and academic papers reveals consistent patterns for effective linter feedback loops.

### How leading coding assistants handle linting

| Tool | Approach | Key Features |
|------|----------|--------------|
| **Cursor** | `read_lints` tool in Agent mode | Auto-reads ESLint/Biome after edits, configurable per-model |
| **Aider** | `--lint-cmd` with auto-retry | Built-in support, sends errors to LLM automatically |
| **Claude Code** | PostToolUse hooks | JSON hooks run linter after Edit/Write operations |
| **GitHub Copilot** | Code Review integration | Surfaces CodeQL, ESLint, Pylint in review comments |

### Credo JSON output format

Running `mix credo --format json --strict` produces:

```json
{
  "issues": [
    {
      "category": "refactor",
      "check": "Elixir.Credo.Check.Refactor.NegatedConditionsWithElse",
      "filename": "lib/foo/bar.ex",
      "line_no": 42,
      "column": 5,
      "message": "Avoid negated conditions in if-else blocks.",
      "priority": 12,
      "trigger": "!",
      "scope": "Foo.Bar.validate"
    }
  ]
}
```

For detailed explanations, use `mix credo lib/file.ex:42 --format json` to get `explanation_for_issue` and `related_code` fields.

### Error feedback prompt templates

**Template 1: Standard fix request**

```markdown
The following Credo errors were detected in your code:

## Errors to Fix

| File | Line | Category | Message |
|------|------|----------|---------|
| lib/user.ex | 42 | refactor | Avoid negated conditions in if-else blocks |
| lib/user.ex | 67 | readability | Pipe chain should start with raw value |

## Details

### lib/user.ex:42
**Rule**: Credo.Check.Refactor.NegatedConditionsWithElse
**Code**:
```elixir
41:     user = Repo.get(User, id)
42:     if !is_nil(user) do
43:       {:ok, user}
44:     else
45:       {:error, :not_found}
46:     end
```
**Fix**: Flip the condition and swap the branches.

Please fix ALL errors. I will re-run Credo to verify.
```

**Template 2: Iterative retry with history**

```markdown
Attempt **2 of 3** to fix Credo errors.

## Previous attempt result
- Fixed: 3 errors
- Remaining: 2 errors
- New errors introduced: 0

## Remaining Errors

[REFACTOR] Line 42: Avoid negated conditions in if-else blocks
  - Previous fix attempt: Changed `if !x` to `unless x`
  - Why it failed: `unless` with `else` is also a violation
  - Correct approach: Use `if is_nil(user)` and swap branches

Please provide a different approach to fix these errors.
```

### Retry strategy best practices

Research consistently shows:

- **Max retries: 3-5** iterations (diminishing returns after 4)
- **Temperature escalation**: Start at 0, increment by 0.1 per retry
- **Error signature tracking**: Detect if same error persists with same approach
- **Progress detection**: Stop if error count isn't decreasing
- **Batch related errors**: Group by file, send 5-10 errors per request
- **Prioritize by severity**: Compilation → Type → Security → Style

---

## Elixir implementation

### Complete validation pipeline

```elixir
defmodule CredoValidator.Pipeline do
  @moduledoc """
  Multi-stage validation pipeline for LLM-generated Elixir code.
  Implements pre-validation, caching, and iterative feedback loops.
  """
  
  @max_retries 3
  @cache_ttl_seconds 300
  
  @doc """
  Validate code with full pipeline. Returns structured report for LLM feedback.
  """
  def validate(code_string, opts \\ []) do
    with {:ok, code} <- stage_syntax_check(code_string),
         {:ok, code} <- stage_quick_checks(code),
         result <- stage_full_credo(code, opts) do
      build_report(result)
    end
  end
  
  @doc """
  Run validation loop with LLM feedback until clean or max retries.
  """
  def validate_with_retry(code_string, fix_callback, opts \\ []) do
    do_validate_loop(code_string, fix_callback, opts, 0, [])
  end
  
  defp do_validate_loop(code, _fix_callback, _opts, attempt, history) 
       when attempt >= @max_retries do
    {:max_retries_exceeded, %{
      attempts: attempt,
      history: Enum.reverse(history),
      final_code: code
    }}
  end
  
  defp do_validate_loop(code, fix_callback, opts, attempt, history) do
    case validate(code, opts) do
      %{valid: true} = report ->
        {:ok, %{code: code, attempts: attempt, report: report}}
      
      %{valid: false, issues: issues} = report ->
        # Check for progress
        previous_count = history |> List.first() |> get_in([:issue_count]) || 999
        current_count = length(issues)
        
        if current_count >= previous_count and attempt > 0 do
          # No progress, try different approach or escalate
          {:stuck, %{
            code: code,
            attempts: attempt,
            remaining_issues: issues
          }}
        else
          # Get fix from LLM
          feedback = format_for_llm(issues, attempt)
          fixed_code = fix_callback.(code, feedback, attempt)
          
          entry = %{attempt: attempt, issue_count: current_count, issues: issues}
          do_validate_loop(fixed_code, fix_callback, opts, attempt + 1, [entry | history])
        end
    end
  end
  
  # Stage 1: Syntax validation
  defp stage_syntax_check(code_string) do
    case Code.string_to_quoted(code_string) do
      {:ok, _ast} -> {:ok, code_string}
      {:error, {line, msg, token}} ->
        {:syntax_error, %{line: line, message: msg, token: token}}
    end
  end
  
  # Stage 2: Quick pattern-based checks
  defp stage_quick_checks(code) do
    issues = []
    |> check_io_inspect(code)
    |> check_pipe_chain_start(code)
    |> check_module_doc(code)
    
    {:ok, code, issues}
  end
  
  # Stage 3: Full Credo analysis
  defp stage_full_credo(code, opts) do
    cache_key = :crypto.hash(:sha256, code) |> Base.encode16()
    
    case CredoCache.get(cache_key) do
      {:ok, cached} -> cached
      :miss ->
        result = run_credo(code, opts)
        CredoCache.put(cache_key, result, @cache_ttl_seconds)
        result
    end
  end
  
  defp run_credo(code, opts) do
    temp_file = Path.join(System.tmp_dir!(), "credo_#{:erlang.unique_integer()}.ex")
    File.write!(temp_file, code)
    
    args = ["credo", temp_file, "--format", "json"] ++
           if(opts[:strict], do: ["--strict"], else: [])
    
    try do
      case System.cmd("mix", args, stderr_to_stdout: true) do
        {output, 0} -> %{valid: true, issues: [], raw: output}
        {output, _} -> parse_credo_json(output)
      end
    after
      File.rm(temp_file)
    end
  end
  
  defp parse_credo_json(json_string) do
    case Jason.decode(json_string) do
      {:ok, %{"issues" => issues}} ->
        parsed = Enum.map(issues, fn issue ->
          %{
            category: String.to_atom(issue["category"]),
            check: issue["check"],
            message: issue["message"],
            line_no: issue["line_no"],
            column: issue["column"],
            priority: issue["priority"],
            trigger: issue["trigger"]
          }
        end)
        %{valid: Enum.empty?(parsed), issues: parsed}
      
      _ -> %{valid: true, issues: []}
    end
  end
  
  # Quick checks
  defp check_io_inspect(issues, code) do
    if String.contains?(code, "IO.inspect") do
      [%{category: :warning, message: "Remove IO.inspect before production", 
         check: "QuickCheck.IoInspect"} | issues]
    else
      issues
    end
  end
  
  defp check_pipe_chain_start(issues, code) do
    # Detect pipes starting with function calls
    if Regex.match?(~r/\w+\([^)]+\)\s*\n?\s*\|>/, code) do
      [%{category: :refactor, message: "Pipe chain should start with raw value",
         check: "QuickCheck.PipeChainStart"} | issues]
    else
      issues
    end
  end
  
  defp check_module_doc(issues, code) do
    if String.contains?(code, "defmodule") and 
       not String.contains?(code, "@moduledoc") do
      [%{category: :design, message: "Add @moduledoc to module",
         check: "QuickCheck.ModuleDoc"} | issues]
    else
      issues
    end
  end
  
  # Report building
  defp build_report(result) when is_tuple(result), do: result
  defp build_report({:ok, code, quick_issues}) do
    %{valid: Enum.empty?(quick_issues), issues: quick_issues, code: code}
  end
  defp build_report(%{} = result), do: result
  
  defp format_for_llm(issues, attempt) do
    header = "Attempt #{attempt + 1} of #{@max_retries}. Fix these Credo errors:\n\n"
    
    body = issues
    |> Enum.sort_by(& &1.priority, :desc)
    |> Enum.map(fn issue ->
      "[#{issue.category |> to_string() |> String.upcase()}] Line #{issue.line_no}: #{issue.message}"
    end)
    |> Enum.join("\n")
    
    header <> body
  end
end
```

### ETS-based caching module

```elixir
defmodule CredoCache do
  @moduledoc "ETS cache for Credo validation results with TTL."
  
  use GenServer
  
  @table :credo_validation_cache
  
  def start_link(_), do: GenServer.start_link(__MODULE__, [], name: __MODULE__)
  
  def init(_) do
    :ets.new(@table, [:set, :public, :named_table, read_concurrency: true])
    {:ok, %{}}
  end
  
  def get(key) do
    case :ets.lookup(@table, key) do
      [{^key, value, expires}] when expires > System.system_time(:second) ->
        {:ok, value}
      _ -> :miss
    end
  end
  
  def put(key, value, ttl_seconds) do
    expires = System.system_time(:second) + ttl_seconds
    :ets.insert(@table, {key, value, expires})
    :ok
  end
end
```

---

## Integration architecture

The complete system operates as a three-stage pipeline with caching at each layer:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         CODE GENERATION REQUEST                              │
└─────────────────────────────────────────────────────────────────────────────┘
                                    │
                                    ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│  LAYER 1: SYSTEM PROMPT INJECTION                                           │
│  ┌──────────────────┐  ┌────────────────────┐  ┌─────────────────────────┐  │
│  │ Static Credo     │  │ Project .credo.exs │  │ Condensed Style Guide   │  │
│  │ Rule Embeddings  │  │ Configuration      │  │ (~600 words)            │  │
│  └──────────────────┘  └────────────────────┘  └─────────────────────────┘  │
└─────────────────────────────────────────────────────────────────────────────┘
                                    │
                                    ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│  LAYER 2: ONTOLOGY/RAG CONTEXT INJECTION                                    │
│  ┌──────────────────┐  ┌────────────────────┐  ┌─────────────────────────┐  │
│  │ SPARQL Query:    │  │ SPARQL Query:      │  │ RAG Retrieval:          │  │
│  │ Naming Patterns  │  │ Module Structure   │  │ Similar Functions       │  │
│  │ (cached 1hr)     │  │ (cached 1hr)       │  │ (3-5 few-shot examples) │  │
│  └──────────────────┘  └────────────────────┘  └─────────────────────────┘  │
└─────────────────────────────────────────────────────────────────────────────┘
                                    │
                                    ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│  LLM CODE GENERATION                                                        │
└─────────────────────────────────────────────────────────────────────────────┘
                                    │
                                    ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│  LAYER 3: POST-GENERATION VALIDATION LOOP                                   │
│                                                                             │
│  ┌────────────┐    ┌─────────────┐    ┌────────────┐    ┌──────────────┐   │
│  │ Syntax     │───▶│ Quick       │───▶│ Full Credo │───▶│ Format with  │   │
│  │ Check      │    │ Pattern     │    │ Analysis   │    │ mix format   │   │
│  │ (<1ms)     │    │ Checks      │    │ (cached)   │    │              │   │
│  └────────────┘    │ (<10ms)     │    │ (100-500ms)│    └──────────────┘   │
│                    └─────────────┘    └────────────┘                        │
│                           │                 │                               │
│                           ▼                 ▼                               │
│                    ┌─────────────────────────────────┐                      │
│                    │  Issues Found?                  │                      │
│                    │  ├─ No  → Return Valid Code     │                      │
│                    │  └─ Yes → Retry Loop (max 3)    │                      │
│                    └─────────────────────────────────┘                      │
│                                    │                                        │
│                                    ▼                                        │
│                    ┌─────────────────────────────────┐                      │
│                    │  Format Errors for LLM          │                      │
│                    │  → Send to LLM for Fix          │                      │
│                    │  → Re-validate                  │                      │
│                    │  → Repeat until clean or max    │                      │
│                    └─────────────────────────────────┘                      │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Performance targets

| Stage | Target Latency | Caching |
|-------|---------------|---------|
| Syntax check | <1ms | None needed |
| Quick pattern checks | <10ms | None needed |
| Full Credo (cached) | <5ms | ETS, 5min TTL |
| Full Credo (uncached) | 100-500ms | By code hash |
| Ontology queries | <50ms | ETS, 1hr TTL |

---

## Key recommendations from existing assistants

Research across Cursor, Aider, Claude Code, and GitHub Copilot reveals these proven patterns:

1. **Cursor's model-specific instructions**: "After substantive edits, use read_lints tool to check recently edited files. If you've introduced errors, fix them if you can easily figure out how."

2. **Aider's wrapper pattern**: For auto-formatters, wrap in a script that runs twice to distinguish formatting changes from real errors.

3. **Claude Code's hooks approach**: PostToolUse hooks that automatically run linters after file modifications provide the cleanest integration.

4. **GitHub Copilot's confidence scores**: Include confidence ratings with fix suggestions to help users decide whether to accept.

5. **Universal finding**: LLMs self-correct effectively **only with external tool feedback**—self-evaluation without linters/compilers is unreliable.

### Avoiding infinite loops

All researched tools struggle with infinite fix loops. Implement these safeguards:

- Hard limit of 3-5 retry attempts
- Track error signatures to detect repeated failures
- Monitor if error count is decreasing between attempts
- Temperature escalation (0 → 0.1 → 0.2) for varied outputs
- Graceful escalation to human review for unfixable issues

---

## Conclusion

This hybrid prevention system combines static prompt rules, dynamic ontology queries, and iterative feedback loops to minimize Credo violations in LLM-generated Elixir code. The **condensed style guide** targets the most commonly violated rules with LLM-optimized formatting. The **SPARQL queries** extract project-specific patterns ensuring consistency with existing code. The **validation pipeline** implements battle-tested patterns from leading coding assistants with proper retry limits and progress tracking.

The architecture prioritizes **prevention over correction**—each earlier layer reduces load on later stages. By embedding Credo rules in system prompts and injecting project context before generation, the system prevents the majority of violations from ever occurring, reserving the more expensive feedback loop for edge cases that slip through.
